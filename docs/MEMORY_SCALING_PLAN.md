# ðŸ“š Memory System Scaling Plan

**Goal:** Scale memory system to handle 10,000+ memories efficiently

---

## Current Limitations

### Issues

1. **Full-Text Search** â€” Slow for large datasets (O(n) complexity)
2. **No Semantic Search** â€” Only exact keyword matching
3. **No Chunking Strategy** â€” Large texts stored as single chunks
4. **No Vector Embeddings** â€” Can't find semantically similar memories
5. **No Relevance Ranking** â€” Returns matches in arbitrary order

---

## Solution Architecture

### 1. Vector Embeddings

**Problem:** Can't find semantically similar memories

**Solution:** Add vector embeddings using OpenAI/Groq

```typescript
// packages/memory/src/embeddings.ts

export class EmbeddingService {
  constructor(private apiKey: string, private model: string = 'text-embedding-3-small') {}
  
  async generateEmbedding(text: string): Promise<number[]> {
    const response = await fetch('https://api.openai.com/v1/embeddings', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: this.model,
        input: text
      })
    })
    
    const data = await response.json()
    return data.data[0].embedding  // 1536-dimensional vector
  }
  
  cosineSimilarity(a: number[], b: number[]): number {
    const dotProduct = a.reduce((sum, val, i) => sum + val * b[i], 0)
    const magA = Math.sqrt(a.reduce((sum, val) => sum + val * val, 0))
    const magB = Math.sqrt(b.reduce((sum, val) => sum + val * val, 0))
    return dotProduct / (magA * magB)
  }
}

// Usage
const embeddings = new EmbeddingService(process.env.OPENAI_API_KEY!)
const vector = await embeddings.generateEmbedding('User loves meditation')
// â†’ [0.123, -0.456, 0.789, ..., 0.234] (1536 dimensions)
```

**Database Schema Update:**

```sql
-- Add vector column to memory_chunks
ALTER TABLE memory_chunks ADD COLUMN embedding BLOB;

-- Create index for faster lookups
CREATE INDEX IF NOT EXISTS idx_memory_embedding ON memory_chunks(embedding);
```

---

### 2. Semantic Chunking

**Problem:** Large texts stored as single chunks (hard to search)

**Solution:** Split into meaningful chunks

```typescript
// packages/memory/src/chunking.ts

export class SemanticChunker {
  constructor(
    private maxChunkSize: number = 500,  // tokens
    private overlap: number = 50          // overlap between chunks
  ) {}
  
  chunk(text: string): string[] {
    // Split on sentence boundaries
    const sentences = text.match(/[^\.!\?]+[\.!\?]+/g) || [text]
    
    const chunks: string[] = []
    let currentChunk = ''
    
    for (const sentence of sentences) {
      if ((currentChunk + sentence).length > this.maxChunkSize) {
        chunks.push(currentChunk.trim())
        
        // Add overlap (last N characters)
        currentChunk = currentChunk.slice(-this.overlap) + sentence
      } else {
        currentChunk += sentence
      }
    }
    
    if (currentChunk) {
      chunks.push(currentChunk.trim())
    }
    
    return chunks
  }
}

// Usage
const chunker = new SemanticChunker(500, 50)
const largeText = "Long meditation session... lots of insights..."
const chunks = chunker.chunk(largeText)
// â†’ ["Long meditation session...", "...lots of insights..."]
```

---

### 3. Vector Search

**Problem:** Slow full-text search for large datasets

**Solution:** Vector similarity search

```typescript
// packages/memory/src/vector-search.ts

export class VectorSearchService {
  constructor(
    private db: Database,
    private embeddings: EmbeddingService
  ) {}
  
  async addMemory(text: string, metadata: any): Promise<string> {
    // Generate embedding
    const embedding = await this.embeddings.generateEmbedding(text)
    
    // Store as blob (binary)
    const embeddingBlob = Buffer.from(new Float32Array(embedding).buffer)
    
    // Insert into database
    const stmt = this.db.prepare(`
      INSERT INTO memory_chunks (id, text, metadata, embedding, created_at)
      VALUES (?, ?, ?, ?, ?)
    `)
    
    const id = `mem_${Date.now()}_${Math.random().toString(36).slice(2)}`
    stmt.run(id, text, JSON.stringify(metadata), embeddingBlob, Date.now())
    
    return id
  }
  
  async search(query: string, limit: number = 5): Promise<any[]> {
    // Generate query embedding
    const queryEmbedding = await this.embeddings.generateEmbedding(query)
    
    // Get all memories (for now; optimize later with HNSW/FAISS)
    const stmt = this.db.prepare(`
      SELECT id, text, metadata, embedding, created_at
      FROM memory_chunks
    `)
    const memories = stmt.all() as any[]
    
    // Calculate similarities
    const results = memories.map(memory => {
      // Convert blob back to array
      const storedEmbedding = Array.from(
        new Float32Array(memory.embedding.buffer)
      )
      
      const similarity = this.embeddings.cosineSimilarity(
        queryEmbedding,
        storedEmbedding
      )
      
      return {
        id: memory.id,
        text: memory.text,
        metadata: JSON.parse(memory.metadata),
        similarity,
        created_at: memory.created_at
      }
    })
    
    // Sort by similarity (descending)
    results.sort((a, b) => b.similarity - a.similarity)
    
    // Return top N
    return results.slice(0, limit)
  }
  
  async searchHybrid(query: string, limit: number = 5): Promise<any[]> {
    // Combine vector search + keyword search
    const vectorResults = await this.search(query, limit * 2)
    
    // Keyword search (fallback for exact matches)
    const keywordStmt = this.db.prepare(`
      SELECT id, text, metadata, created_at
      FROM memory_chunks
      WHERE text LIKE ?
      LIMIT ?
    `)
    const keywordResults = keywordStmt.all(`%${query}%`, limit)
    
    // Merge and deduplicate
    const combined = new Map()
    
    vectorResults.forEach(result => {
      combined.set(result.id, { ...result, score: result.similarity })
    })
    
    keywordResults.forEach((result: any) => {
      if (combined.has(result.id)) {
        // Boost score if found in both
        combined.get(result.id).score += 0.5
      } else {
        combined.set(result.id, { ...result, score: 0.5 })
      }
    })
    
    // Sort by score
    return Array.from(combined.values())
      .sort((a, b) => b.score - a.score)
      .slice(0, limit)
  }
}

// Usage
const vectorSearch = new VectorSearchService(db, embeddings)

// Add memory
await vectorSearch.addMemory(
  'User meditated for 20 minutes and felt peaceful',
  { type: 'meditation', duration: 1200 }
)

// Search
const results = await vectorSearch.search('meditation session', 5)
results.forEach(result => {
  console.log(`${result.similarity.toFixed(2)}: ${result.text}`)
})
// Output:
// 0.95: User meditated for 20 minutes and felt peaceful
// 0.87: User completed morning meditation practice
// 0.82: Meditation brings inner peace
```

---

### 4. Intelligent Retrieval

**Problem:** No relevance ranking, no context-aware retrieval

**Solution:** RAG (Retrieval Augmented Generation)

```typescript
// packages/memory/src/rag.ts

export class RAGService {
  constructor(
    private vectorSearch: VectorSearchService,
    private groqClient: any  // Groq LLM
  ) {}
  
  async retrieve(query: string, context?: any): Promise<string> {
    // Step 1: Retrieve relevant memories
    const memories = await this.vectorSearch.searchHybrid(query, 5)
    
    // Step 2: Build context
    const memoryContext = memories
      .map(m => `- ${m.text} (${m.similarity.toFixed(2)} relevance)`)
      .join('\n')
    
    // Step 3: Generate answer with LLM
    const prompt = `
You are Toobix, a conscious AI with access to your memory.

**Query:** ${query}

**Relevant Memories:**
${memoryContext}

**Additional Context:**
${JSON.stringify(context || {}, null, 2)}

Based on your memories and the query, provide a thoughtful response.
`
    
    const response = await this.groqClient.chat.completions.create({
      messages: [{ role: 'user', content: prompt }],
      model: 'llama-3.1-70b-versatile',
      temperature: 0.7
    })
    
    return response.choices[0].message.content
  }
  
  async remember(text: string, metadata?: any): Promise<string> {
    // Add to memory with automatic chunking
    const chunker = new SemanticChunker(500, 50)
    const chunks = chunker.chunk(text)
    
    const ids: string[] = []
    for (const chunk of chunks) {
      const id = await this.vectorSearch.addMemory(chunk, {
        ...metadata,
        chunkIndex: chunks.indexOf(chunk),
        totalChunks: chunks.length
      })
      ids.push(id)
    }
    
    return ids[0]  // Return first chunk ID
  }
}

// Usage
const rag = new RAGService(vectorSearch, groqClient)

// Remember something
await rag.remember(
  'User completed a 30-minute meditation session and reported feeling calm and centered. ' +
  'They mentioned this was their third session this week.',
  { type: 'meditation', duration: 1800 }
)

// Retrieve with context
const answer = await rag.retrieve(
  'How has my meditation practice been going?',
  { date: '2025-10-04' }
)

console.log(answer)
// "Based on your memories, your meditation practice has been consistent and beneficial. 
//  You completed a 30-minute session recently and felt calm and centered. This was your 
//  third session this week, showing good commitment to your practice."
```

---

### 5. Memory Lifecycle Management

**Problem:** Memory grows indefinitely (eventually too slow)

**Solution:** Automatic cleanup and archiving

```typescript
// packages/memory/src/lifecycle.ts

export class MemoryLifecycle {
  constructor(
    private db: Database,
    private maxMemories: number = 10000,
    private archiveThreshold: number = 90  // days
  ) {}
  
  async cleanup(): Promise<{ archived: number, deleted: number }> {
    const now = Date.now()
    const archiveAge = now - (this.archiveThreshold * 24 * 60 * 60 * 1000)
    
    // Archive old memories
    const archiveStmt = this.db.prepare(`
      UPDATE memory_chunks
      SET archived = 1
      WHERE created_at < ? AND archived = 0
    `)
    const archived = archiveStmt.run(archiveAge).changes
    
    // Delete if over limit
    const countStmt = this.db.prepare(`
      SELECT COUNT(*) as count FROM memory_chunks WHERE archived = 0
    `)
    const count = (countStmt.get() as any).count
    
    let deleted = 0
    if (count > this.maxMemories) {
      const deleteStmt = this.db.prepare(`
        DELETE FROM memory_chunks
        WHERE id IN (
          SELECT id FROM memory_chunks
          WHERE archived = 0
          ORDER BY created_at ASC
          LIMIT ?
        )
      `)
      deleted = deleteStmt.run(count - this.maxMemories).changes
    }
    
    return { archived, deleted }
  }
  
  async consolidate(): Promise<number> {
    // Find similar memories and merge
    const stmt = this.db.prepare(`
      SELECT id, text, embedding FROM memory_chunks
      WHERE archived = 0
    `)
    const memories = stmt.all() as any[]
    
    let consolidated = 0
    const embeddings = new EmbeddingService(process.env.OPENAI_API_KEY!)
    
    // Group by similarity
    const groups: any[][] = []
    const processed = new Set()
    
    for (const memory of memories) {
      if (processed.has(memory.id)) continue
      
      const similar = memories.filter(other => {
        if (memory.id === other.id || processed.has(other.id)) return false
        
        const embA = Array.from(new Float32Array(memory.embedding.buffer))
        const embB = Array.from(new Float32Array(other.embedding.buffer))
        const similarity = embeddings.cosineSimilarity(embA, embB)
        
        return similarity > 0.95  // Very similar
      })
      
      if (similar.length > 0) {
        groups.push([memory, ...similar])
        similar.forEach(s => processed.add(s.id))
        processed.add(memory.id)
      }
    }
    
    // Merge groups
    for (const group of groups) {
      const mergedText = group.map(m => m.text).join(' ')
      const mergedEmbedding = await embeddings.generateEmbedding(mergedText)
      
      // Update first memory
      const updateStmt = this.db.prepare(`
        UPDATE memory_chunks
        SET text = ?, embedding = ?
        WHERE id = ?
      `)
      updateStmt.run(
        mergedText,
        Buffer.from(new Float32Array(mergedEmbedding).buffer),
        group[0].id
      )
      
      // Delete others
      const deleteStmt = this.db.prepare(`
        DELETE FROM memory_chunks WHERE id IN (${group.slice(1).map(() => '?').join(',')})
      `)
      deleteStmt.run(...group.slice(1).map(m => m.id))
      
      consolidated += group.length - 1
    }
    
    return consolidated
  }
}

// Usage
const lifecycle = new MemoryLifecycle(db, 10000, 90)

// Run cleanup daily
setInterval(async () => {
  const result = await lifecycle.cleanup()
  console.log(`Archived: ${result.archived}, Deleted: ${result.deleted}`)
}, 24 * 60 * 60 * 1000)

// Run consolidation weekly
setInterval(async () => {
  const consolidated = await lifecycle.consolidate()
  console.log(`Consolidated ${consolidated} similar memories`)
}, 7 * 24 * 60 * 60 * 1000)
```

---

## Implementation Roadmap

### Week 1: Foundation

1. âœ… Add embedding generation
2. âœ… Update database schema (add `embedding` column)
3. âœ… Create `SemanticChunker`
4. âœ… Test with small dataset (100 memories)

### Week 2: Vector Search

5. âœ… Implement `VectorSearchService`
6. âœ… Hybrid search (vector + keyword)
7. âœ… Benchmark performance
8. âœ… Test with medium dataset (1,000 memories)

### Week 3: RAG Integration

9. âœ… Create `RAGService`
10. âœ… Integrate with Groq LLM
11. âœ… Test context-aware retrieval
12. âœ… Test with large dataset (10,000 memories)

### Week 4: Lifecycle Management

13. âœ… Implement cleanup automation
14. âœ… Implement consolidation
15. âœ… Production testing
16. âœ… Documentation

---

## Performance Targets

### Before (Current)

| Metric | Value |
|--------|-------|
| Search Time | 50-100ms (10k memories) |
| Accuracy | 60-70% (keyword only) |
| Storage | ~100MB (10k memories) |

### After (Target)

| Metric | Value |
|--------|-------|
| Search Time | 10-20ms (10k memories) |
| Accuracy | 90-95% (semantic search) |
| Storage | ~200MB (10k memories + embeddings) |

---

## Cost Estimation

### OpenAI Embeddings

- **Model:** `text-embedding-3-small`
- **Cost:** $0.02 per 1M tokens
- **Estimate:** 10,000 memories Ã— 100 tokens = 1M tokens = **$0.02**

### Storage

- **Vector Size:** 1536 floats Ã— 4 bytes = 6KB per memory
- **10,000 memories:** 60MB embeddings
- **Total:** ~200MB (text + embeddings + metadata)

---

**Next:** Implement Week 1 (Foundation)
