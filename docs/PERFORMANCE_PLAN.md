# üöÄ Performance Optimization Plan

**Goal:** Optimize Toobix-Unified for production-scale performance

---

## Current Bottlenecks

### Identified Issues

1. **API Calls** ‚Äî No caching, every request hits external APIs
2. **Memory Queries** ‚Äî Full-text search on every retrieval
3. **Event Processing** ‚Äî Synchronous 6-step pipeline
4. **Database** ‚Äî No connection pooling, no prepared statements
5. **Module Loading** ‚Äî All modules loaded at startup

---

## Optimization Strategies

### 1. Caching Layer

**Problem:** Repeated API calls for same data

**Solution:** Multi-level caching

```typescript
// packages/core/src/performance/cache.ts

export class CacheLayer {
  private memoryCache: Map<string, any>
  private ttl: Map<string, number>
  
  constructor(private maxSize: number = 1000, private defaultTTL: number = 300000) {
    this.memoryCache = new Map()
    this.ttl = new Map()
  }
  
  get(key: string): any | null {
    const expiry = this.ttl.get(key)
    if (expiry && Date.now() > expiry) {
      this.memoryCache.delete(key)
      this.ttl.delete(key)
      return null
    }
    return this.memoryCache.get(key) || null
  }
  
  set(key: string, value: any, ttl?: number): void {
    if (this.memoryCache.size >= this.maxSize) {
      // LRU eviction
      const firstKey = this.memoryCache.keys().next().value
      this.memoryCache.delete(firstKey)
      this.ttl.delete(firstKey)
    }
    
    this.memoryCache.set(key, value)
    this.ttl.set(key, Date.now() + (ttl || this.defaultTTL))
  }
  
  clear(): void {
    this.memoryCache.clear()
    this.ttl.clear()
  }
}

// Usage
const cache = new CacheLayer(1000, 300000) // 1000 items, 5min TTL

// Cache API responses
const cachedValue = cache.get('groq:response:' + hash)
if (cachedValue) return cachedValue

const response = await groq.chat(...)
cache.set('groq:response:' + hash, response)
```

**Benefits:**
- ‚ö° 10-100x faster for repeated queries
- üí∞ Reduced API costs
- üîÑ Better user experience

---

### 2. Lazy Loading

**Problem:** All modules loaded at startup (slow initialization)

**Solution:** Load modules on-demand

```typescript
// packages/bridge/src/performance/lazy-loader.ts

export class LazyModuleLoader {
  private modules: Map<string, any>
  private loaders: Map<string, () => Promise<any>>
  
  constructor() {
    this.modules = new Map()
    this.loaders = new Map()
  }
  
  register(name: string, loader: () => Promise<any>): void {
    this.loaders.set(name, loader)
  }
  
  async get(name: string): Promise<any> {
    if (this.modules.has(name)) {
      return this.modules.get(name)
    }
    
    const loader = this.loaders.get(name)
    if (!loader) {
      throw new Error(`Module ${name} not registered`)
    }
    
    console.log(`‚è≥ Loading module: ${name}...`)
    const module = await loader()
    this.modules.set(name, module)
    console.log(`‚úÖ Loaded module: ${name}`)
    
    return module
  }
}

// Usage in Bridge
const loader = new LazyModuleLoader()

loader.register('soul', async () => {
  const { SoulService } = await import('../../soul/src/index.ts')
  return new SoulService(this.db)
})

loader.register('ethics', async () => {
  const { EthicsService } = await import('../../ethics/src/index.ts')
  return new EthicsService(this.db)
})

// Load on first use
const soul = await loader.get('soul')
```

**Benefits:**
- ‚ö° 5-10x faster startup
- üíæ Lower memory footprint
- üîß Better modularity

---

### 3. Batch Processing

**Problem:** Events processed one-by-one (slow for bulk operations)

**Solution:** Batch event processing

```typescript
// packages/core/src/pipeline/batch-processor.ts

export class BatchEventProcessor {
  private queue: any[]
  private processing: boolean
  
  constructor(
    private pipeline: EventPipeline,
    private batchSize: number = 10,
    private flushInterval: number = 1000
  ) {
    this.queue = []
    this.processing = false
    
    // Auto-flush every interval
    setInterval(() => this.flush(), this.flushInterval)
  }
  
  async add(event: any): Promise<string> {
    return new Promise((resolve, reject) => {
      this.queue.push({ event, resolve, reject })
      
      if (this.queue.length >= this.batchSize) {
        this.flush()
      }
    })
  }
  
  private async flush(): Promise<void> {
    if (this.processing || this.queue.length === 0) return
    
    this.processing = true
    const batch = this.queue.splice(0, this.batchSize)
    
    console.log(`üì¶ Processing batch of ${batch.length} events...`)
    
    try {
      const results = await Promise.all(
        batch.map(({ event }) => this.pipeline.processEvent(event))
      )
      
      batch.forEach(({ resolve }, index) => {
        resolve(results[index].eventId)
      })
    } catch (error) {
      batch.forEach(({ reject }) => reject(error))
    } finally {
      this.processing = false
    }
  }
}

// Usage
const batchProcessor = new BatchEventProcessor(pipeline, 10, 1000)

// Add events to queue
await batchProcessor.add({ type: 'event1', ... })
await batchProcessor.add({ type: 'event2', ... })
// ... auto-flushes when 10 events or 1 second
```

**Benefits:**
- ‚ö° 5-10x faster bulk operations
- üíæ Reduced database writes
- üîÑ Better resource utilization

---

### 4. Database Optimization

**Problem:** No prepared statements, no connection pooling

**Solution:** Prepared statements + indexing

```typescript
// packages/core/src/db/optimized-queries.ts

export class OptimizedDB {
  private db: Database
  private statements: Map<string, Statement>
  
  constructor(dbPath: string) {
    this.db = new Database(dbPath)
    this.statements = new Map()
    this.createIndexes()
  }
  
  private createIndexes(): void {
    // Index frequently queried columns
    this.db.exec(`
      CREATE INDEX IF NOT EXISTS idx_story_events_type ON story_events(type);
      CREATE INDEX IF NOT EXISTS idx_story_events_created ON story_events(created_at);
      CREATE INDEX IF NOT EXISTS idx_memory_chunks_text ON memory_chunks(text);
    `)
  }
  
  prepare(name: string, sql: string): Statement {
    if (this.statements.has(name)) {
      return this.statements.get(name)!
    }
    
    const stmt = this.db.prepare(sql)
    this.statements.set(name, stmt)
    return stmt
  }
  
  // Optimized queries
  getRecentEvents(limit: number = 10) {
    const stmt = this.prepare('recentEvents', `
      SELECT * FROM story_events
      ORDER BY created_at DESC
      LIMIT ?
    `)
    return stmt.all(limit)
  }
  
  searchMemory(query: string, limit: number = 5) {
    const stmt = this.prepare('searchMemory', `
      SELECT * FROM memory_chunks
      WHERE text LIKE ?
      LIMIT ?
    `)
    return stmt.all(`%${query}%`, limit)
  }
}
```

**Benefits:**
- ‚ö° 2-5x faster queries
- üíæ Lower memory usage
- üîí Better SQL injection protection

---

### 5. Async Pipeline

**Problem:** 6-step pipeline is synchronous (blocks on each step)

**Solution:** Parallel execution where possible

```typescript
// packages/core/src/pipeline/async-pipeline.ts

export class AsyncEventPipeline extends EventPipeline {
  async processEvent(event: any): Promise<any> {
    // Step 1: Validation (required, sync)
    const validation = this.validateEvent(event)
    if (!validation.valid) {
      return { success: false, errors: validation.errors }
    }
    
    // Step 2: Ethics check (required)
    const ethicsResult = await this.checkEthics(event)
    if (!ethicsResult.isEthical) {
      return { success: false, reason: 'Ethics violation' }
    }
    
    // Steps 3-6: Run in parallel where possible
    const [valuesResult, reflectionResult] = await Promise.all([
      // Step 3: Values update (optional)
      this.updateValues(event),
      
      // Step 4: Reflection (optional, can run in parallel)
      event.requiresReflection 
        ? this.triggerReflection(event)
        : Promise.resolve(null)
    ])
    
    // Steps 5-6: Story + Memory (sequential, Story is authoritative)
    const storyEvent = await this.logToStory(event)
    const memoryId = await this.storeInMemory(event, storyEvent.id)
    
    return {
      success: true,
      eventId: storyEvent.id,
      memoryId,
      ethicsScore: ethicsResult.score
    }
  }
}
```

**Benefits:**
- ‚ö° 2-3x faster event processing
- üîÑ Better CPU utilization
- üìä Maintained consistency (Story still authoritative)

---

## Implementation Priority

### Phase 1: Quick Wins (Week 1)

1. ‚úÖ **Caching Layer** ‚Äî Biggest impact, easiest to implement
2. ‚úÖ **Database Indexes** ‚Äî Simple SQL, major speedup
3. ‚úÖ **Prepared Statements** ‚Äî Security + performance

### Phase 2: Architectural (Week 2-3)

4. ‚úÖ **Lazy Loading** ‚Äî Refactor module initialization
5. ‚úÖ **Async Pipeline** ‚Äî Refactor EventPipeline
6. ‚úÖ **Batch Processing** ‚Äî Add batch processor

### Phase 3: Advanced (Week 4+)

7. ‚è≥ **CDN for Static Assets**
8. ‚è≥ **Redis Cache** (optional, if needed)
9. ‚è≥ **Load Balancing** (multiple Bridge instances)

---

## Benchmarks

### Before Optimization

| Operation | Time | Memory |
|-----------|------|--------|
| Bridge Startup | 2-3s | 150MB |
| Event Processing | 100-150ms | +10MB |
| Memory Search | 50-100ms | +5MB |
| API Call | 500-1000ms | +2MB |

### After Optimization (Target)

| Operation | Time | Memory |
|-----------|------|--------|
| Bridge Startup | <500ms | 80MB |
| Event Processing | 30-50ms | +2MB |
| Memory Search | 10-20ms | +1MB |
| API Call (cached) | 1-5ms | +0.5MB |

**Total Improvement:** 5-10x faster, 50% less memory

---

## Monitoring

Add performance metrics:

```typescript
// packages/core/src/performance/metrics.ts

export class PerformanceMetrics {
  private metrics: Map<string, number[]>
  
  constructor() {
    this.metrics = new Map()
  }
  
  track(name: string, duration: number): void {
    if (!this.metrics.has(name)) {
      this.metrics.set(name, [])
    }
    this.metrics.get(name)!.push(duration)
  }
  
  getStats(name: string) {
    const values = this.metrics.get(name) || []
    if (values.length === 0) return null
    
    const sorted = values.sort((a, b) => a - b)
    return {
      count: values.length,
      min: sorted[0],
      max: sorted[sorted.length - 1],
      median: sorted[Math.floor(sorted.length / 2)],
      avg: values.reduce((a, b) => a + b, 0) / values.length,
      p95: sorted[Math.floor(sorted.length * 0.95)],
      p99: sorted[Math.floor(sorted.length * 0.99)]
    }
  }
  
  report(): string {
    let report = '\nüìä PERFORMANCE METRICS\n\n'
    
    for (const [name, _] of this.metrics) {
      const stats = this.getStats(name)!
      report += `${name}:\n`
      report += `  Count: ${stats.count}\n`
      report += `  Avg: ${stats.avg.toFixed(2)}ms\n`
      report += `  P95: ${stats.p95.toFixed(2)}ms\n`
      report += `  P99: ${stats.p99.toFixed(2)}ms\n\n`
    }
    
    return report
  }
}

// Usage
const metrics = new PerformanceMetrics()

const start = Date.now()
await pipeline.processEvent(event)
metrics.track('event_processing', Date.now() - start)

console.log(metrics.report())
```

---

**Next:** Implement Phase 1 (Caching + DB Optimization)
